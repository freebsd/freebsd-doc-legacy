<?xml version="1.0" encoding="iso-8859-1"?>
<!--
     The FreeBSD Documentation Project
     $FreeBSD$
-->

<chapter id="filesystems">
  <chapterinfo>
    <authorgroup>
      <author>
	<firstname>Tom</firstname>
	<surname>Rhodes</surname>
	<contrib>Written by </contrib>
      </author>
    </authorgroup>
  </chapterinfo>

  <title>File Systems Support</title>

  <sect1 id="filesystems-synopsis">
    <title>Synopsis</title>

    <indexterm><primary>File Systems</primary></indexterm>
    <indexterm>
      <primary>File Systems Support</primary>
      <see>File Systems</see>
    </indexterm>

    <para>File systems are an integral part of any operating system.
      They allow users to upload and store files, provide access
      to data, and make hard drives useful.  Different operating
      systems differ in their native file system.  Traditionally, the
      native &os; file system has been the Unix File System
      <acronym>UFS</acronym> which has been modernized as
      <acronym>UFS2</acronym>.  Since &os;&nbsp;7.0, the Z File
      System <acronym>ZFS</acronym> is also available as a native file
      system.</para>

    <para>In addition to its native file systems, &os; supports a
      multitude of other file systems so that data from other
      operating systems can be accessed locally, such as data stored
      on locally attached <acronym>USB</acronym> storage devices,
      flash drives, and hard disks.  This includes support for the
      &linux; Extended File System (<acronym>EXT</acronym>) and the
      &microsoft; New Technology File System
      (<acronym>NTFS</acronym>).</para>

    <para>There are different levels of &os; support for the various
      file systems.  Some require a kernel module to be loaded and
      others may require a toolset to be installed.  Some non-native
      file system support is full read-write while others are
      read-only.</para>

    <para>After reading this chapter, you will know:</para>

    <itemizedlist>
      <listitem>
	<para>The difference between native and supported file
	  systems.</para>
      </listitem>

      <listitem>
	<para>Which file systems are supported by &os;.</para>
      </listitem>

      <listitem>
	<para>How to enable, configure, access, and make use of
	  non-native file systems.</para>
      </listitem>
    </itemizedlist>

    <para>Before reading this chapter, you should:</para>

    <itemizedlist>
      <listitem>
	<para>Understand &unix; and <link
	    linkend="basics">&os; basics</link>.</para>
      </listitem>

      <listitem>
	<para>Be familiar with the basics of <link
	    linkend="kernelconfig">kernel configuration and
	    compilation</link>.</para>
      </listitem>

      <listitem>
	<para>Feel comfortable <link linkend="ports">installing
	    software</link> in &os;.</para>
      </listitem>

      <listitem>
	<para>Have some familiarity with <link
	    linkend="disks">disks</link>, storage, and device names in
	  &os;.</para>
      </listitem>
    </itemizedlist>
  </sect1>

  <sect1 id="filesystems-zfs">
    <title>The Z File System (ZFS)</title>

    <para>The Z&nbsp;file system, originally developed by &sun;,
      is designed to future proof the file system by removing many of
      the arbitrary limits imposed on previous file systems.  ZFS
      allows continuous growth of the pooled storage by adding
      additional devices.  ZFS allows you to create many file systems
      (in addition to block devices) out of a single shared pool of
      storage.  Space is allocated as needed, so all remaining free
      space is available to each file system in the pool.  It is also
      designed for maximum data integrity, supporting data snapshots,
      multiple copies, and cryptographic checksums.  It uses a
      software data replication model, known as
      <acronym>RAID</acronym>-Z. <acronym>RAID</acronym>-Z provides
      redundancy similar to hardware <acronym>RAID</acronym>, but is
      designed to prevent data write corruption and to overcome some
      of the limitations of hardware <acronym>RAID</acronym>.</para>

    <sect2 id="filesystems-zfs-term">
      <title>ZFS Features and Terminology</title>

      <para>ZFS is a fundamentally different file system because it
	is more than just a file system.  ZFS combines the roles of
	file system and volume manager, enabling additional storage
	devices to be added to a live system and having the new space
	available on all of the existing file systems in that pool
	immediately.  By combining the traditionally separate roles,
	ZFS is able to overcome previous limitations that prevented
	RAID groups being able to grow.  Each top level device in a
	zpool is called a vdev, which can be a simple disk or a RAID
	transformation such as a mirror or RAID-Z array.  ZFS file
	systems (called datasets), each have access to the combined
	free space of the entire pool.  As blocks are allocated the
	free space in the pool available to of each file system is
	decreased.  This approach avoids the common pitfall with
	extensive partitioning where free space becomes fragmentated
	across the partitions.</para>

      <informaltable pgwide="1">
	<tgroup cols="2">
	  <tbody>
	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-zpool"/>zpool</entry>

	      <entry>A storage pool is the most basic building block
		of ZFS.  A pool is made up of one or more vdevs, the
		underlying devices that store the data.  A pool is
		then used to create one or more file systems
		(datasets) or block devices (volumes).  These datasets
		and volumes share the pool of remaining free space.
		Each pool is uniquely identified by a name and a
		<acronym>GUID</acronym>.  The zpool also controls the
		version number and therefore the features available
		for use with ZFS.
		<note><para>&os; 9.0 and 9.1 include
		  support for ZFS version 28.  Future versions use ZFS
		  version 5000 with feature flags.  This allows
		  greater cross-compatibility with other
		  implementations of ZFS.
		</para></note></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-vdev"/>vdev&nbsp;Types</entry>

	      <entry>A zpool is made up of one or more vdevs, which
		themselves can be a single disk or a group of disks,
		in the case of a RAID transform.  When multiple vdevs
		are used, ZFS spreads data across the vdevs to
		increase performance and maximize usable space.
		<itemizedlist>
		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-disk"/>
		      <emphasis>Disk</emphasis> - The most basic type
		      of vdev is a standard block device.  This can be
		      an entire disk (such as
		      <devicename><replaceable>/dev/ada0</replaceable></devicename>
		      or
		      <devicename><replaceable>/dev/da0</replaceable></devicename>)
		      or a partition
		      (<devicename><replaceable>/dev/ada0p3</replaceable></devicename>).
		      Contrary to the Solaris documentation, on &os;
		      there is no performance penalty for using a
		      partition rather than an entire disk.</para>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-file"/>
		      <emphasis>File</emphasis> - In addition to
		      disks, ZFS pools can be backed by regular files,
		      this is especially useful for testing and
		      experimentation.  Use the full path to the file
		      as the device path in the zpool create command.
		      All vdevs must be atleast 128&nbsp;MB in
		      size.</para>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-mirror"/>
		      <emphasis>Mirror</emphasis> - When creating a
		      mirror, specify the <literal>mirror</literal>
		      keyword followed by the list of member devices
		      for the mirror.  A mirror consists of two or
		      more devices, all data will be written to all
		      member devices.  A mirror vdev will only hold as
		      much data as its smallest member.  A mirror vdev
		      can withstand the failure of all but one of its
		      members without losing any data.</para>

		    <note>
		      <para>
			A regular single disk vdev can be
			upgraded to a mirror vdev at any time using
			the <command>zpool</command> <link
			linkend="filesystems-zfs-zpool-attach">attach</link>
			command.</para>
		    </note>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-raidz"/>
		      <emphasis><acronym>RAID</acronym>-Z</emphasis> -
		      ZFS implements RAID-Z, a variation on standard
		      RAID-5 that offers better distribution of parity
		      and eliminates the "RAID-5 write hole" in which
		      the data and parity information become
		      inconsistent after an unexpected restart.  ZFS
		      supports 3 levels of RAID-Z which provide
		      varying levels of redundancy in exchange for
		      decreasing levels of usable storage.  The types
		      are named RAID-Z1 through Z3 based on the number
		      of parity devinces in the array and the number
		      of disks that the pool can operate
		      without.</para>

		    <para>In a RAID-Z1 configuration with 4 disks,
		      each 1&nbsp;TB, usable storage will be 3&nbsp;TB
		      and the pool will still be able to operate in
		      degraded mode with one faulted disk.  If an
		      additional disk goes offline before the faulted
		      disk is replaced and resilvered, all data in the
		      pool can be lost.</para>

		    <para>In a RAID-Z3 configuration with 8 disks of
		      1&nbsp;TB, the volume would provide 5TB of
		      usable space and still be able to operate with
		      three faulted disks.  Sun recommends no more
		      than 9 disks in a single vdev.  If the
		      configuration has more disks, it is recommended
		      to divide them into separate vdevs and the pool
		      data will be striped across them.</para>

		    <para>A configuration of 2 RAID-Z2 vdevs
		      consisting of 8 disks each would create
		      something similar to a RAID 60 array.  A RAID-Z
		      group's storage capacity is approximately the
		      size of the smallest disk, multiplied by the
		      number of non-parity disks.  4x 1&nbsp;TB disks
		      in Z1 has an effective size of approximately
		      3&nbsp;TB, and a 8x 1&nbsp;TB array in Z3 will
		      yeild 5&nbsp;TB of usable space.</para>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-spare"/>
		      <emphasis>Spare</emphasis> - ZFS has a special
		      pseudo-vdev type for keeping track of available
		      hot spares.  Note that installed hot spares are
		      not deployed automatically; they must manually
		      be configured to replace the failed device using
		      the zfs replace command.</para>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-log"/>
		      <emphasis>Log</emphasis> - ZFS Log Devices, also
		      known as ZFS Intent Log (<acronym>ZIL</acronym>)
		      move the intent log from the regular pool
		      devices to a dedicated device.  The ZIL
		      accelerates synchronous transactions by using
		      storage devices (such as
		      <acronym>SSD</acronym>s) that are faster
		      compared to those used for the main pool.  When
		      data is being written and the application
		      requests a guarantee that the data has been
		      safely stored, the data is written to the faster
		      ZIL storage, then later flushed out to the
		      regular disks, greatly reducing the latency of
		      synchronous writes.  Log devices can be
		      mirrored, but RAID-Z is not supported.  When
		      specifying multiple log devices writes will be
		      load balanced across all devices.</para>
		  </listitem>

		  <listitem>
		    <para><anchor
		      id="filesystems-zfs-term-vdev-cache"/>
		      <emphasis>Cache</emphasis> - Adding a cache vdev
		      to a zpool will add the storage of the cache to
		      the L2ARC.  Cache devices cannot be mirrored.
		      Since a cache device only stores additional
		      copies of existing data, there is no risk of
		      data loss.</para>
		  </listitem>
		</itemizedlist></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-arc"/>Adaptive Replacement
		Cache (<acronym>ARC</acronym>)</entry>

	      <entry>ZFS uses an Adaptive Replacement Cache
		(<acronym>ARC</acronym>), rather than a more
		traditional Least Recently Used
		(<acronym>LRU</acronym>) cache.  An
		<acronym>LRU</acronym> cache is a simple list of items
		in the cache sorted by when each object was most
		recently used; new items are added to the top of the
		list and once the cache is full items from the bottom
		of the list are evicted to make room for more active
		objects.  An <acronym>ARC</acronym> consists of four
		lists; the Most Recently Used (<acronym>MRU</acronym>)
		and Most Frequently Used (<acronym>MFU</acronym>)
		objects, plus a ghost list for each.  These ghost
		lists tracks recently evicted objects to provent them
		being added back to the cache.  This increases the
		cache hit ratio by avoiding objects that have a
		history of only being used occasionally.  Another
		advantage of using both an <acronym>MRU</acronym> and
		<acronym>MFU</acronym> is that scanning an entire
		filesystem would normally evict all data from an
		<acronym>MRU</acronym> or <acronym>LRU</acronym> cache
		in favor of this freshly accessed content.  In the
		case of <acronym>ZFS</acronym> since there is also an
		<acronym>MFU</acronym> that only tracks the most
		frequently used objects, the cache of the most
		commonly accessed blocks remains.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-l2arc"/>L2ARC</entry>

	      <entry>The <acronym>L2ARC</acronym> is the second level
		of the <acronym>ZFS</acronym> caching system.  The
		primary <acronym>ARC</acronym> is stored in
		<acronym>RAM</acronym>, however since the amount of
		available <acronym>RAM</acronym> is often limited,
		<acronym>ZFS</acronym> can also make use of <link
		linkend="filesystems-zfs-term-vdev-cache">cache</link>
		vdevs.  Solid State Disks (<acronym>SSD</acronym>s)
		are often used as these cache devices due to their
		higher speed and lower latency compared to traditional
		spinning disks.  An L2ARC is entirely optional, but
		having one will significantly increase read speeds for
		files that are cached on the <acronym>SSD</acronym>
		instead of having to be read from the regular spinning
		disks.  The L2ARC can also speed up <link
		linkend="filesystems-zfs-term-deduplication">deduplication</link>
		since a <acronym>DDT</acronym> that does not fit in
		<acronym>RAM</acronym> but does fit in the
		<acronym>L2ARC</acronym> will be much faster than if
		the <acronym>DDT</acronym> had to be read from disk.
		The rate at which data is added to the cache devices
		is limited to prevent prematurely wearing out the
		<acronym>SSD</acronym> with too many writes.  Until
		the cache is full (the first block has been evicted to
		make room), writing to the <acronym>L2ARC</acronym> is
		limited to the sum of the write limit and the boost
		limit, then after that limited to the write limit.  A
		pair of sysctl values control these rate limits;
		<literal>vfs.zfs.l2arc_write_max</literal> controls
		how many bytes are written to the cache per second,
		while <literal>vfs.zfs.l2arc_write_boost</literal>
		adds to this limit during the "Turbo Warmup Phase"
		(Write Boost).</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-cow"/>Copy-On-Write</entry>

	      <entry>Unlike a traditional file system, when data is
		overwritten on ZFS the new data is written to a
		different block rather than overwriting the old data
		in place.  Only once this write is complete is the
		metadata then updated to point to the new location of
		the data.  This means that in the event of a shorn
		write (a system crash or power loss in the middle of
		writing a file) the entire original contents of the
		file are still available and the incomplete write is
		discarded.  This also means that ZFS does not require
		a fsck after an unexpected shutdown.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-dataset"/>Dataset</entry>

	      <entry></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-volum"/>Volume</entry>

	      <entry>In additional to regular file systems (datasets),
		ZFS can also create volumes, which are block devices.
		Volumes have many of the same features, including
		copy-on-write, snapshots, clones and
		checksumming.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-snapshot"/>Snapshot</entry>

	      <entry>The <link
		  linkend="filesystems-zfs-term-cow">copy-on-write</link>
		design of ZFS allows for nearly instantaneous
		consistent snapshots with arbitrary names.  After
		taking a snapshot of a dataset (or a recursive
		snapshot of a parent dataset that will include all
		child datasets), new data is written to new blocks (as
		described above), however the old blocks are not
		reclaimed as free space.  There are then two versions
		of the file system, the snapshot (what the file system
		looked like before) and the live file system; however
		no additional space is used.  As new data is written
		to the live file system, new blocks are allocated to
		store this data.  The apparent size of the snapshot
		will grow as the blocks are no longer used in the live
		file system, but only in the snapshot.  These
		snapshots can be mounted (read only) to allow for the
		recovery of previous versions of files.  It is also
		possible to <link
		linkend="filesystems-zfs-zfs-snapshot">rollback</link>
		a live file system to a specific snapshot, undoing any
		changes that took place after the snapshot was taken.
		Each block in the zpool has a reference counter which
		indicates how many snapshots, clones, datasets or
		volumes make use of that block.  As files and
		snapshots are deleted, the reference count is
		decremented; once a block is no longer referenced, it
		is reclaimed as free space.  Snapshots can also be
		marked with a <link
		linkend="filesystems-zfs-zfs-snapshot">hold</link>,
		once a snapshot is held, any attempt to destroy it
		will return an EBUY error.  Each snapshot can have
		multiple holds, each with a unique name.  The <link
		linkend="filesystems-zfs-zfs-snapshot">release</link>
		command removes the hold so the snapshot can then be
		deleted.  Snapshots can be taken on volumes, however
		they can only be cloned or rolled back, not mounted
		independently.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-clone"/>Clone</entry>

	      <entry>Snapshots can also be cloned; a clone is a
		writable version of a snapshot, allowing the file
		system to be forked as a new dataset.  As with a
		snapshot, a clone initially consumes no additional
		space, only as new data is written to a clone and new
		blocks are allocated does the apparent size of the
		clone grow.  As blocks are overwritten in the cloned
		file system or volume, the reference count on the
		previous block is decremented.  The snapshot upon
		which a clone is based cannot be deleted because the
		clone is dependeant upon it (the snapshot is the
		parent, and the clone is the child).  Clones can be
		<literal>promoted</literal>, reversing this
		dependeancy, making the clone the parent and the
		previous parent the child.  This operation requires no
		additional space, however it will change the way the
		used space is accounted.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-checksum"/>Checksum</entry>

	      <entry>Every block that is allocated is also checksummed
		(which algorithm is used is a per dataset property,
		see: zfs set).  ZFS transparently validates the
		checksum of each block as it is read, allowing ZFS to
		detect silent corruption.  If the data that is read
		does not match the expected checksum, ZFS will attempt
		to recover the data from any available redundancy
		(mirrors, RAID-Z).  You can trigger the validation of
		all checksums using the <link
		linkend="filesystems-zfs-term-scrub">scrub</link>
		command.  The available checksum algorithms include:
		<itemizedlist>
		  <listitem><para>fletcher2</para></listitem>
		  <listitem><para>fletcher4</para></listitem>
		  <listitem><para>sha256</para></listitem>
		</itemizedlist> The fletcher algorithms are faster,
		but sha256 is a strong cryptographic hash and has a
		much lower chance of a collisions at the cost of some
		performance.  Checksums can be disabled but it is
		inadvisable.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-compression"/>Compression</entry>

	      <entry>Each dataset in ZFS has a compression property,
		which defaults to off.  This property can be set to
		one of a number of compression algorithms, which will
		cause all new data that is written to this dataset to
		be compressed as it is written.  In addition to the
		reduction in disk usage, this can also increase read
		and write throughput, as only the smaller compressed
		version of the file needs to be read or
		written.<note>
		  <para>LZ4 compression is only available after &os;
		    9.2</para>
		</note></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-deduplication"/>Deduplication</entry>

	      <entry>ZFS has the ability to detect duplicate blocks of
		data as they are written (thanks to the checksumming
		feature).  If deduplication is enabled, instead of
		writing the block a second time, the reference count
		of the existing block will be increased, saving
		storage space.  In order to do this, ZFS keeps a
		deduplication table (<acronym>DDT</acronym>) in
		memory, containing the list of unique checksums, the
		location of that block and a reference count.  When
		new data is written, the checksum is calculated and
		compared to the list.  If a match is found, the data
		is considered to be a duplicate.  When deduplication
		is enabled, the checksum algorithm is changed to
		<acronym>SHA256</acronym> to provide a secure
		cryptographic hash.  ZFS deduplication is tunable; if
		dedup is on, then a matching checksum is assumed to
		mean that the data is identical.  If dedup is set to
		verify, then the data in the two blocks will be
		checked byte-for-byte to ensure it is actually
		identical and if it is not, the hash collision will be
		noted by ZFS and the two blocks will be stored
		separately.  Due to the nature of the
		<acronym>DDT</acronym>, having to store the hash of
		each unique block, it consumes a very large amount of
		memory (a general rule of thumb is 5-6&nbsp;GB of ram
		per 1&nbsp;TB of deduplicated data).  In situations
		where it is not practical to have enough
		<acronym>RAM</acronym> to keep the entire DDT in
		memory, performance will suffer greatly as the DDT
		will need to be read from disk before each new block
		is written.  Deduplication can make use of the L2ARC
		to store the DDT, providing a middle ground between
		fast system memory and slower disks.  It is advisable
		to consider using ZFS compression instead, which often
		provides nearly as much space savings without the
		additional memory requirement.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-scrub"/>Scrub</entry>

	      <entry>In place of a consistency check like fsck, ZFS
		has the <literal>scrub</literal> command, which reads
		all data blocks stored on the pool and verifies their
		checksums them against the known good checksums stored
		in the metadata.  This periodic check of all the data
		stored on the pool ensures the recovery of any
		corrupted blocks before they are needed.  A scrub is
		not required after an unclean shutdown, but it is
		recommended that you run a scrub at least once each
		quarter.  ZFS compares the checksum for each block as
		it is read in the normal course of use, but a scrub
		operation makes sure even infrequently used blocks are
		checked for silent corruption.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-quota"/>Dataset
		Quota</entry>

	      <entry>ZFS provides very fast and accurate dataset, user
		and group space accounting in addition to quotes and
		space reservations.  This gives the administrator fine
		grained control over how space is allocated and allows
		critical file systems to reserve space to ensure other
		file systems do not take all of the free space.
		<para>ZFS supports different types of quotas: the
		  dataset quota, the <link
		  linkend="filesystems-zfs-term-refquota">reference
		  quota (<acronym>refquota</acronym>)</link>, the
		  <link linkend="filesystems-zfs-term-userquota">user
		  quota</link>, and the <link
		  linkend="filesystems-zfs-term-groupquota">
		    group quota</link>.</para>

		<para>Quotas limit the amount of space that a dataset
		  and all of its descendants (snapshots of the
		  dataset, child datasets and the snapshots of those
		  datasets) can consume.</para>

		<note>
		  <para>Quotas cannot be set on volumes, as the
		    <literal>volsize</literal> property acts as an
		    implicit quota.</para>
		</note></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-refquota"/>Reference
		Quota</entry>

	      <entry>A reference quota limits the amount of space a
		dataset can consume by enforcing a hard limit on the
		space used.  However, this hard limit includes only
		space that the dataset references and does not include
		space used by descendants, such as file systems or
		snapshots.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-userquota"/>User
		  Quota</entry>

	      <entry>User quotas are useful to limit the amount of
		space that can be used by the specified user.</entry>
	    </row>

	    <row>
	      <entry valign="top">
		<anchor id="filesystems-zfs-term-groupquota"/>Group
		  Quota</entry>

	      <entry>The group quota limits the amount of space that a
		specified group can consume.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-reservation"/>Dataset
		  Reservation</entry>

	      <entry>The <literal>reservation</literal> property makes
		it possible to guaranteed a minimum amount of space
		for the use of a specific dataset and its descendants.
		This means that if a 10&nbsp;GB reservation is set on
		<filename>storage/home/bob</filename>, if another
		dataset tries to use all of the free space, at least
		10&nbsp;GB of space is reserved for this dataset.  If
		a snapshot is taken of
		<filename>storage/home/bob</filename>, the space used
		by that snapshot is counted against the reservation.
		The <link
		linkend="filesystems-zfs-term-refreservation">refreservation</link>
		property works in a similar way, except it
		<emphasis>excludes</emphasis> descendants, such as
		snapshots.
		<para>Reservations of any sort are useful
		  in many situations, such as planning and testing the
		  suitability of disk space allocation in a new
		  system, or ensuring that enough space is available
		  on file systems for audio logs or system recovery
		  procedures and files.</para></entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-refreservation"/>Reference
		  Reservation</entry>

	      <entry>The <literal>refreservation</literal> property
		makes it possible to guaranteed a minimum amount of
		space for the use of a specific dataset
		<emphasis>excluding</emphasis> its descendants.  This
		means that if a 10&nbsp;GB reservation is set on
		<filename>storage/home/bob</filename>, if another
		dataset tries to use all of the free space, at least
		10&nbsp;GB of space is reserved for this dataset.  In
		contrast to a regular <link
		linkend="filesystems-zfs-term-reservation">reservation</link>,
		space used by snapshots and decendant datasets is not
		counted against the reservation.  As an example, if a
		snapshot was taken of
		<filename>storage/home/bob</filename>, enough disk
		space would have to exist outside of the
		<literal>refreservation</literal> amount for the
		operation to succeed because descendants of the main
		data set are not counted by the
		<literal>refreservation</literal> amount and so do not
		encroach on the space set.</entry>
	    </row>

	    <row>
	      <entry valign="top"><anchor
		id="filesystems-zfs-term-resilver"/>Resilver</entry>

	      <entry></entry>
	    </row>

	  </tbody>
	</tgroup>
      </informaltable>
    </sect2>

    <sect2 id="filesystems-zfs-differences">
      <title>What Makes ZFS Different</title>

      <para></para>
    </sect2>

    <sect2 id="filesystems-zfs-quickstart">
      <title><acronym>ZFS</acronym> Quick Start Guide</title>

      <para>There is a start up mechanism that allows &os; to
	mount <acronym>ZFS</acronym> pools during system
	initialization.  To set it, issue the following
	commands:</para>

      <screen>&prompt.root; <userinput>echo 'zfs_enable="YES"' &gt;&gt; /etc/rc.conf</userinput>
&prompt.root; <userinput>service zfs start</userinput></screen>

      <para>The examples in this section assume three
	<acronym>SCSI</acronym> disks with the device names
	<devicename><replaceable>da0</replaceable></devicename>,
	<devicename><replaceable>da1</replaceable></devicename>,
	and <devicename><replaceable>da2</replaceable></devicename>.
	Users of <acronym>SATA</acronym> hardware should instead use
	<devicename><replaceable>ada</replaceable></devicename>
	device names.</para>

      <sect3>
	<title>Single Disk Pool</title>

	<para>To create a simple, non-redundant <acronym>ZFS</acronym>
	  pool using a single disk device, use
	  <command>zpool</command>:</para>

	<screen>&prompt.root; <userinput>zpool create <replaceable>example</replaceable> <replaceable>/dev/da0</replaceable></userinput></screen>

	<para>To view the new pool, review the output of
	  <command>df</command>:</para>

	<screen>&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</screen>

	<para>This output shows that the <literal>example</literal>
	  pool has been created and <emphasis>mounted</emphasis>.  It
	  is now accessible as a file system.  Files may be created
	  on it and users can browse it, as seen in the following
	  example:</para>

	<screen>&prompt.root; <userinput>cd /example</userinput>
&prompt.root; <userinput>ls</userinput>
&prompt.root; <userinput>touch testfile</userinput>
&prompt.root; <userinput>ls -al</userinput>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</screen>

	<para>However, this pool is not taking advantage of any
	  <acronym>ZFS</acronym> features.  To create a dataset on
	  this pool with compression enabled:</para>

	<screen>&prompt.root; <userinput>zfs create example/compressed</userinput>
&prompt.root; <userinput>zfs set compression=gzip example/compressed</userinput></screen>

	<para>The <literal>example/compressed</literal> dataset is now
	  a <acronym>ZFS</acronym> compressed file system.  Try
	  copying some large files to <filename
	    class="directory">/example/compressed</filename>.</para>

	<para>Compression can be disabled with:</para>

	<screen>&prompt.root; <userinput>zfs set compression=off example/compressed</userinput></screen>

	<para>To unmount a file system, issue the following command
	  and then verify by using <command>df</command>:</para>

	<screen>&prompt.root; <userinput>zfs umount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</screen>

	<para>To re-mount the file system to make it accessible
	  again, and verify with <command>df</command>:</para>

	<screen>&prompt.root; <userinput>zfs mount example/compressed</userinput>
&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</screen>

	<para>The pool and file system may also be observed by viewing
	  the output from <command>mount</command>:</para>

	<screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
example on /example (zfs, local)
example/data on /example/data (zfs, local)
example/compressed on /example/compressed (zfs, local)</screen>

	<para><acronym>ZFS</acronym> datasets, after creation, may be
	  used like any file systems.  However, many other features
	  are available which can be set on a per-dataset basis.  In
	  the following example, a new file system,
	  <literal>data</literal> is created.  Important files will be
	  stored here, the file system is set to keep two copies of
	  each data block:</para>

	<screen>&prompt.root; <userinput>zfs create example/data</userinput>
&prompt.root; <userinput>zfs set copies=2 example/data</userinput></screen>

	<para>It is now possible to see the data and space utilization
	  by issuing <command>df</command>:</para>

	<screen>&prompt.root; <userinput>df</userinput>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</screen>

	<para>Notice that each file system on the pool has the same
	  amount of available space.  This is the reason for using
	  <command>df</command> in these examples, to show that the
	  file systems use only the amount of space they need and all
	  draw from the same pool.  The <acronym>ZFS</acronym> file
	  system does away with concepts such as volumes and
	  partitions, and allows for several file systems to occupy
	  the same pool.</para>

	<para>To destroy the file systems and then destroy the pool as
	  they are no longer needed:</para>

	<screen>&prompt.root; <userinput>zfs destroy example/compressed</userinput>
&prompt.root; <userinput>zfs destroy example/data</userinput>
&prompt.root; <userinput>zpool destroy example</userinput></screen>

      </sect3>

      <sect3>
	<title><acronym>ZFS</acronym> RAID-Z</title>

	<para>There is no way to prevent a disk from failing.  One
	  method of avoiding data loss due to a failed hard disk is to
	  implement <acronym>RAID</acronym>.  <acronym>ZFS</acronym>
	  supports this feature in its pool design.  RAID-Z pools
	  require 3 or more disks but yield more usable space than
	  mirrored pools.</para>

	<para>To create a <acronym>RAID</acronym>-Z pool, issue the
	  following command and specify the disks to add to the
	  pool:</para>

	<screen>&prompt.root; <userinput>zpool create storage raidz da0 da1 da2</userinput></screen>

	<note>
	  <para>&sun; recommends that the number of devices used in
	    a <acronym>RAID</acronym>-Z configuration is between
	    three and nine.  For environments requiring a single pool
	    consisting of 10 disks or more, consider breaking it up
	    into smaller <acronym>RAID</acronym>-Z groups.  If only
	    two disks are available and redundancy is a requirement,
	    consider using a <acronym>ZFS</acronym> mirror.  Refer to
	    &man.zpool.8; for more details.</para>
	</note>

	<para>This command creates the <literal>storage</literal>
	  zpool.  This may be verified using &man.mount.8; and
	  &man.df.1;.  This command makes a new file system in the
	  pool called <literal>home</literal>:</para>

	<screen>&prompt.root; <userinput>zfs create storage/home</userinput></screen>

	<para>It is now possible to enable compression and keep extra
	  copies of directories and files using the following
	  commands:</para>

	<screen>&prompt.root; <userinput>zfs set copies=2 storage/home</userinput>
&prompt.root; <userinput>zfs set compression=gzip storage/home</userinput></screen>

	<para>To make this the new home directory for users, copy the
	  user data to this directory, and create the appropriate
	  symbolic links:</para>

	<screen>&prompt.root; <userinput>cp -rp /home/* /storage/home</userinput>
&prompt.root; <userinput>rm -rf /home /usr/home</userinput>
&prompt.root; <userinput>ln -s /storage/home /home</userinput>
&prompt.root; <userinput>ln -s /storage/home /usr/home</userinput></screen>

	<para>Users should now have their data stored on the freshly
	  created <filename
	    class="directory">/storage/home</filename>.  Test by
	  adding a new user and logging in as that user.</para>

	<para>Try creating a snapshot which may be rolled back
	  later:</para>

	<screen>&prompt.root; <userinput>zfs snapshot storage/home@08-30-08</userinput></screen>

	<para>Note that the snapshot option will only capture a real
	  file system, not a home directory or a file.  The
	  <literal>@</literal> character is a delimiter used between
	  the file system name or the volume name.  When a user's
	  home directory gets trashed, restore it with:</para>

	<screen>&prompt.root; <userinput>zfs rollback storage/home@08-30-08</userinput></screen>

	<para>To get a list of all available snapshots, run
	  <command>ls</command> in the file system's
	  <filename class="directory">.zfs/snapshot</filename>
	  directory.  For example, to see the previously taken
	  snapshot:</para>

	<screen>&prompt.root; <userinput>ls /storage/home/.zfs/snapshot</userinput></screen>

	<para>It is possible to write a script to perform regular
	  snapshots on user data.  However, over time, snapshots
	  may consume a great deal of disk space.  The previous
	  snapshot may be removed using the following command:</para>

	<screen>&prompt.root; <userinput>zfs destroy storage/home@08-30-08</userinput></screen>

	<para>After testing, <filename
	    class="directory">/storage/home</filename> can be made the
	  real <filename class="directory">/home</filename> using
	  this command:</para>

	<screen>&prompt.root; <userinput>zfs set mountpoint=/home storage/home</userinput></screen>

	<para>Run <command>df</command> and
	  <command>mount</command> to confirm that the system now
	  treats the file system as the real
	  <filename class="directory">/home</filename>:</para>

	<screen>&prompt.root; <userinput>mount</userinput>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
storage on /storage (zfs, local)
storage/home on /home (zfs, local)
&prompt.root; <userinput>df</userinput>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</screen>

	<para>This completes the <acronym>RAID</acronym>-Z
	  configuration.  To get status updates about the file systems
	  created during the nightly &man.periodic.8; runs, issue the
	  following command:</para>

	<screen>&prompt.root; <userinput>echo 'daily_status_zfs_enable="YES"' &gt;&gt; /etc/periodic.conf</userinput></screen>
      </sect3>

      <sect3>
	<title>Recovering <acronym>RAID</acronym>-Z</title>

	<para>Every software <acronym>RAID</acronym> has a method of
	  monitoring its <literal>state</literal>.  The status of
	  <acronym>RAID</acronym>-Z devices may be viewed with the
	  following command:</para>

	<screen>&prompt.root; <userinput>zpool status -x</userinput></screen>

	<para>If all pools are healthy and everything is normal, the
	  following message will be returned:</para>

	<screen>all pools are healthy</screen>

	<para>If there is an issue, perhaps a disk has gone offline,
	  the pool state will look similar to:</para>

	<screen>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device with
	'zpool replace'.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

	<para>This indicates that the device was previously taken
	  offline by the administrator using the following
	  command:</para>

	<screen>&prompt.root; <userinput>zpool offline storage da1</userinput></screen>

	<para>It is now possible to replace
	  <devicename>da1</devicename> after the system has been
	  powered down.  When the system is back online, the following
	  command may issued to replace the disk:</para>

	<screen>&prompt.root; <userinput>zpool replace storage da1</userinput></screen>

	<para>From here, the status may be checked again, this time
	  without the <option>-x</option> flag to get state
	  information:</para>

	<screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

	<para>As shown from this example, everything appears to be
	  normal.</para>
      </sect3>

      <sect3>
	<title>Data Verification</title>

	<para><acronym>ZFS</acronym> uses checksums to verify the
	  integrity of stored data.  These are enabled automatically
	  upon creation of file systems and may be disabled using the
	  following command:</para>

	<screen>&prompt.root; <userinput>zfs set checksum=off storage/home</userinput></screen>

	<para>Doing so is <emphasis>not</emphasis> recommended as
	  checksums take very little storage space and are used to
	  check data integrity using checksum verification in a
	  process is known as <quote>scrubbing.</quote>  To verify the
	  data integrity of the <literal>storage</literal> pool, issue
	  this command:</para>

	<screen>&prompt.root; <userinput>zpool scrub storage</userinput></screen>

	<para>This process may take considerable time depending on
	  the amount of data stored.  It is also very
	  <acronym>I/O</acronym> intensive, so much so that only one
	  scrub may be run at any given time.  After the scrub has
	  completed, the status is updated and may be viewed by
	  issuing a status request:</para>

	<screen>&prompt.root; <userinput>zpool status storage</userinput>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</screen>

	<para>The completion time is displayed and helps to ensure
	  data integrity over a long period of time.</para>

	<para>Refer to &man.zfs.8; and &man.zpool.8; for other
	  <acronym>ZFS</acronym> options.</para>
      </sect3>
    </sect2>

    <sect2 id="filesystems-zfs-zpool">
      <title><command>zpool</command> Administration</title>

      <para></para>

      <sect3 id="filesystems-zfs-zpool-create">
	<title>Creating &amp; Destroying Storage Pools</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-attach">
	<title>Adding &amp; Removing Devices</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-resilver">
	<title>Dealing with Failed Devices</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-import">
	<title>Importing &amp; Exporting Pools</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-upgrade">
	<title>Upgrading a Storage Pool</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-status">
	<title>Checking the Status of a Pool</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-iostat">
	<title>Performance Monitoring</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zpool-split">
	<title>Splitting a Storage Pool</title>

	<para></para>
      </sect3>
    </sect2>

    <sect2 id="filesystems-zfs-zfs">
      <title><command>zfs</command> Administration</title>

      <para></para>

      <sect3 id="filesystems-zfs-zfs-create">
	<title>Creating &amp; Destroying Datasets</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-volume">
	<title>Creating &amp; Destroying Volumes</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-rename">
	<title>Renaming a Dataset</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-set">
	<title>Setting Dataset Properties</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-snapshot">
	<title>Managing Snapshots</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-clones">
	<title>Managing Clones</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-send">
	<title>ZFS Replication</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-quota">
	<title>Dataset, User and Group Quotes</title>

	<para>To enforce a dataset quota of 10&nbsp;GB for
	  <filename>storage/home/bob</filename>, use the
	  following:</para>

	<screen>&prompt.root; <userinput>zfs set quota=10G storage/home/bob</userinput></screen>

	<para>To enforce a reference quota of 10&nbsp;GB for
	  <filename>storage/home/bob</filename>, use the
	  following:</para>

	<screen>&prompt.root; <userinput>zfs set refquota=10G storage/home/bob</userinput></screen>

	<para>The general
	  format is
	  <literal>userquota@<replaceable>user</replaceable>=<replaceable>size</replaceable></literal>,
	  and the user's name must be in one of the following
	  formats:</para>

	<itemizedlist>
	  <listitem>
	    <para><acronym
		role="Portable Operating System
	      Interface">POSIX</acronym> compatible name such as
	      <replaceable>joe</replaceable>.</para>
	  </listitem>

	  <listitem>
	    <para><acronym
		role="Portable Operating System
		Interface">POSIX</acronym>
	      numeric ID such as
	      <replaceable>789</replaceable>.</para>
	  </listitem>

	  <listitem>
	    <para><acronym role="System Identifier">SID</acronym> name
	      such as
	      <replaceable>joe.bloggs@example.com</replaceable>.</para>
	  </listitem>

	  <listitem>
	    <para><acronym role="System Identifier">SID</acronym>
	      numeric ID such as
	      <replaceable>S-1-123-456-789</replaceable>.</para>
	  </listitem>
	</itemizedlist>

	<para>For example, to enforce a user quota of 50&nbsp;GB
	  for a user named <replaceable>joe</replaceable>, use the
	  following:</para>

	<screen>&prompt.root; <userinput>zfs set userquota@joe=50G</userinput></screen>

	<para>To remove the quota or make sure that one is not set,
	  instead use:</para>

	<screen>&prompt.root; <userinput>zfs set userquota@joe=none</userinput></screen>

	<note>
	  <para>User quota properties are not displayed by
	    <command>zfs get all</command>.
	    Non-<username>root</username> users can only see their own
	    quotas unless they have been granted the
	    <literal>userquota</literal> privilege.  Users with this
	    privilege are able to view and set everyone's
	    quota.</para>
	</note>

	<para>The general format for setting a group quota is:
	  <literal>groupquota@<replaceable>group</replaceable>=<replaceable>size</replaceable></literal>.</para>

	<para>To set the quota for the group
	  <replaceable>firstgroup</replaceable> to 50&nbsp;GB,
	  use:</para>

	<screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=50G</userinput></screen>

	<para>To remove the quota for the group
	  <replaceable>firstgroup</replaceable>, or to make sure that
	  one is not set, instead use:</para>

	<screen>&prompt.root; <userinput>zfs set groupquota@firstgroup=none</userinput></screen>

	<para>As with the user quota property,
	  non-<username>root</username> users can only see the quotas
	  associated with the groups that they belong to.  However,
	  <username>root</username> or a user with the
	  <literal>groupquota</literal> privilege can view and set all
	  quotas for all groups.</para>

	<para>To display the amount of space consumed by each user on
	  the specified filesystem or snapshot, along with any
	  specified quotas, use <command>zfs userspace</command>.
	  For group information, use <command>zfs
	    groupspace</command>.  For more information about
	  supported options or how to display only specific options,
	  refer to &man.zfs.1;.</para>

	<para>Users with sufficient privileges and
	  <username>root</username> can list the quota for
	  <filename>storage/home/bob</filename> using:</para>

	<screen>&prompt.root; <userinput>zfs get quota storage/home/bob</userinput></screen>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-reservation">
	<title>Reservations</title>

	<para></para>

	<para>The general format of the <literal>reservation</literal>
	  property is
	  <literal>reservation=<replaceable>size</replaceable></literal>,
	  so to set a reservation of 10&nbsp;GB on
	  <filename>storage/home/bob</filename>, use:</para>

	<screen>&prompt.root; <userinput>zfs set reservation=10G storage/home/bob</userinput></screen>

	<para>To make sure that no reservation is set, or to remove a
	  reservation, use:</para>

	<screen>&prompt.root; <userinput>zfs set reservation=none storage/home/bob</userinput></screen>

	<para>The same principle can be applied to the
	  <literal>refreservation</literal> property for setting a
	  refreservation, with the general format
	  <literal>refreservation=<replaceable>size</replaceable></literal>.</para>

	<para>To check if any reservations or refreservations exist on
	  <filename>storage/home/bob</filename>, execute one of the
	  following commands:</para>

	<screen>&prompt.root; <userinput>zfs get reservation storage/home/bob</userinput>
&prompt.root; <userinput>zfs get refreservation storage/home/bob</userinput></screen>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-compression">
	<title>Compression</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-deduplication">
	<title>Deduplication</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-zfs-allow">
	<title>Delegated Administration</title>

	<para></para>
      </sect3>
    </sect2>

    <sect2 id="filesystems-zfs-advanced">
      <title>ZFS Advanced Topics</title>

      <sect3 id="filesystems-zfs-advanced-tuning">
	<title>ZFS Tuning</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-advanced-booting">
	<title>Booting Root on ZFS</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-advanced-beadm">
	<title>ZFS Boot Environments</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-advanced-troubleshoot">
	<title>Troubleshooting</title>

	<para></para>
      </sect3>

      <sect3 id="filesystems-zfs-advanced-i386">
	<title>ZFS on i386</title>

	<para>Some of the features provided by <acronym>ZFS</acronym>
	  are RAM-intensive, so some tuning may be required to provide
	  maximum efficiency on systems with limited RAM.</para>

	<sect4>
	  <title>Memory</title>

	  <para>At a bare minimum, the total system memory should be
	    at least one gigabyte.  The amount of recommended RAM
	    depends upon the size of the pool and the ZFS features
	    which are used.  A general rule of thumb is 1GB of RAM
	    for every 1TB of storage.  If the deduplication feature
	    is used, a general rule of thumb is 5GB of RAM per TB of
	    storage to be deduplicated.  While some users successfully
	    use ZFS with less RAM, it is possible that when the system
	    is under heavy load, it may panic due to memory
	    exhaustion.  Further tuning may be required for systems
	    with less than the recommended RAM requirements.</para>
	</sect4>

	<sect4>
	  <title>Kernel Configuration</title>

	  <para>Due to the RAM limitations of the &i386; platform,
	    users using ZFS on the &i386; architecture should add the
	    following option to a custom kernel configuration file,
	    rebuild the kernel, and reboot:</para>

	  <programlisting>options        KVA_PAGES=512</programlisting>

	  <para>This option expands the kernel address space, allowing
	    the <varname>vm.kvm_size</varname> tunable to be pushed
	    beyond the currently imposed limit of 1&nbsp;GB, or the
	    limit of 2&nbsp;GB for <acronym>PAE</acronym>.  To find
	    the most suitable value for this option, divide the
	    desired address space in megabytes by four (4).  In this
	    example, it is <literal>512</literal> for
	    2&nbsp;GB.</para>
	</sect4>

	<sect4>
	  <title>Loader Tunables</title>

	  <para>The <devicename>kmem</devicename> address space can
	    be increased on all &os; architectures.  On a test system
	    with one gigabyte of physical memory, success was achieved
	    with the following options added to
	    <filename>/boot/loader.conf</filename>, and the system
	    restarted:</para>

	  <programlisting>vm.kmem_size="330M"
vm.kmem_size_max="330M"
vfs.zfs.arc_max="40M"
vfs.zfs.vdev.cache.size="5M"</programlisting>

	  <para>For a more detailed list of recommendations for
	    ZFS-related tuning, see <ulink
	      url="http://wiki.freebsd.org/ZFSTuningGuide"></ulink>.</para>
	</sect4>
      </sect3>
    </sect2>

    <sect2 id="filesystems-zfs-links">
      <title>Additional Resources</title>

      <itemizedlist>
	<listitem><para><ulink url="https://wiki.freebsd.org/ZFS">
	    FreeBSD Wiki - ZFS</ulink></para></listitem>
	<listitem><para><ulink
	    url="https://wiki.freebsd.org/ZFSTuningGuide">
	    FreeBSD Wiki - ZFS Tuning</ulink></para></listitem>
	<listitem><para><ulink
	    url="http://wiki.illumos.org/display/illumos/ZFS">
	    Illumos Wiki - ZFS</ulink></para></listitem>
	<listitem><para><ulink
	    url="http://docs.oracle.com/cd/E19253-01/819-5461/index.html">
	    Oracle Solaris ZFS Administration
	    Guide</ulink></para></listitem>
	<listitem><para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide">
	    ZFS Evil Tuning Guide</ulink></para></listitem>
	<listitem><para><ulink
	    url="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">
	    ZFS Best Practices Guide</ulink></para></listitem>
      </itemizedlist>
    </sect2>
  </sect1>

  <sect1 id="filesystems-linux">
    <title>&linux; Filesystems</title>

    <para>This section describes some of the &linux; filesystems
      supported by &os;.</para>

    <sect2>
      <title><acronym>ext2</acronym></title>

      <para>The &man.ext2fs.5; file system kernel implementation has
	been available since &os;&nbsp;2.2.  In &os;&nbsp;8.x and
	earlier, the code is licensed under the
	<acronym>GPL</acronym>.  Since &os;&nbsp;9.0, the code has
	been rewritten and is now <acronym>BSD</acronym>
	licensed.</para>

      <para>The &man.ext2fs.5; driver allows the &os; kernel to both
	read and write to <acronym>ext2</acronym> file systems.</para>

      <para>To access an <acronym>ext2</acronym> file system, first
	load the kernel loadable module:</para>

      <screen>&prompt.root; <userinput>kldload ext2fs</userinput></screen>

      <para>Then, to mount an &man.ext2fs.5; volume located on
	<filename>/dev/ad1s1</filename>:</para>

      <screen>&prompt.root; <userinput>mount -t ext2fs /dev/ad1s1 /mnt</userinput></screen>
    </sect2>

    <sect2>
      <title>XFS</title>

      <para><acronym>XFS</acronym> was originally written by
	<acronym>SGI</acronym> for the <acronym>IRIX</acronym>
	operating system and was then ported to &linux; and
	released under the <acronym>GPL</acronym>.  See
	<ulink url="http://oss.sgi.com/projects/xfs">this page</ulink>
	for more details.  The &os; port was started by Russel
	Cattelan, &a.kan.email;, and &a.rodrigc.email;.</para>

      <para>To load <acronym>XFS</acronym> as a kernel-loadable
	module:</para>

      <screen>&prompt.root; <userinput>kldload xfs</userinput></screen>

      <para>The &man.xfs.5; driver lets the &os; kernel access XFS
	filesystems.  However, only read-only access is supported and
	writing to a volume is not possible.</para>

      <para>To mount a &man.xfs.5; volume located on
	<filename>/dev/ad1s1</filename>:</para>

      <screen>&prompt.root; <userinput>mount -t xfs /dev/ad1s1 /mnt</userinput></screen>

      <para>The <filename role="package">sysutils/xfsprogs</filename>
	port includes the <command>mkfs.xfs</command> which enables
	the creation of <acronym>XFS</acronym> filesystems, plus
	utilities for analyzing and repairing them.</para>

      <para>The <literal>-p</literal> flag to
	<command>mkfs.xfs</command> can be used to create an
	&man.xfs.5; filesystem which is populated with files and other
	metadata.  This can be used to quickly create a read-only
	filesystem which can be tested on &os;.</para>
    </sect2>

    <sect2>
      <title>ReiserFS</title>

      <para>The Reiser file system, ReiserFS, was ported to
	&os; by &a.dumbbell.email;, and has been released under the
	<acronym>GPL</acronym> .</para>

      <para>The ReiserFS driver permits the &os; kernel to access
	ReiserFS file systems and read their contents, but not
	write to them.</para>

      <para>First, the kernel-loadable module needs to be
	loaded:</para>

      <screen>&prompt.root; <userinput>kldload reiserfs</userinput></screen>

      <para>Then, to mount a ReiserFS volume located on
	<filename>/dev/ad1s1</filename>:</para>

      <screen>&prompt.root; <userinput>mount -t reiserfs /dev/ad1s1 /mnt</userinput></screen>
    </sect2>
  </sect1>

  <!--
      XXXTR: stub sections (added later, as needed, as desire,
      after I get opinions from -doc people):

      Still need to discuss native and foreign file systems.

  <sect1>
    <title>Device File System</title>
  </sect1>

  <sect1>
    <title>DOS and NTFS File Systems</title>
    <para>This is a good section for those who transfer files, using
      USB devices, from Windows to FreeBSD and vice-versa.  My camera,
      and many other cameras I have seen default to using FAT16.  There
      is (was?) a kde utility, I think called kamera, that could be used
      to access camera devices.  A section on this would be useful.</para>

    <para>XXXTR: Though!  The disks chapter, covers a bit of this and
      devfs under it's USB devices.  It leaves a lot to be desired though,
      see:
http://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/usb-disks.html
      It may be better to flesh out that section a bit more.  Add the
      word "camera" to it so that others can easily notice.</para>
  </sect1>

  <sect1>
    <title>Linux EXT File System</title>

    <para>Probably NOT as useful as the other two, but it requires
      knowledge of the existence of the tools.  Which are hidden in
      the ports collection.  Most Linux guys would probably only use
      Linux, BSD guys would be smarter and use NFS.</para>
  </sect1>

  <sect1>
    <title>HFS</title>

    <para>I think this is the file system used on Apple OSX.  There are
      tools in the ports collection, and with Apple being a big
      FreeBSD supporter and user of our technologies, surely there
      is enough cross over to cover this?</para>
  </sect1>
  -->

</chapter>
